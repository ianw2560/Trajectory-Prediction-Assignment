\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Trajectory Prediction Code Modifications}

\author{\IEEEauthorblockN{Ian Wallace}
\IEEEauthorblockA{\textit{Department of ECE} \\
\textit{University of Central Florida}\\
Orlando, Florida \\
ia106173@ucf.edu}
}

\maketitle

\section{Introduction}
In this work, we explore three modifications to the Transformer architecture addressing the issues above: (1) an architectural change in the placement of normalization (and other minor adjustments) to improve training stability, (2) making the Q, K, V projections fully non-linear by using learnable multi-layer networks instead of single linear layers, and (3) incorporating a dynamic k-means clustering mechanism within the attention layers to sparsify connections and improve efficiency on long sequences. We implement these modifications and evaluate their impact on model performance and efficiency. The following sections describe each modification in detail and present experimental results comparing the modified models to the standard Transformer baseline.

\section{Modifications}

\subsection{Transformer Architecture}

The first modification to the original trajectory prediction code we performed was changing the model architecture. We kept the overall TUTR design fixed but systematically varied the core Transformer hyperparameters: the embedding size, the number of encoder/decoder layers, the number of attention heads, and the width of the feed-forward blocks. The baseline model uses an embedding size of 256, two encoder/decoder layers, eight attention heads, and a feed-forward expansion factor of 2. This corresponds to the default valuees in the original paper. Starting from this baseline, we defined three architecture variants:

\begin{enumerate}
\item An overall smaller model.
\item A model with a deeper encoder/decoder.
\item A wider model with stronger feed-forward blocks.
\end{enumerate}

The overall smaller model reduces the dimensionality of the token representations and the number of attention heads, effectively shrinking both the self-attention and feed-forward components. For the second configuration, we keep the width of the model the same as the baseline, but double the depth of the encoder and decoder to 256. The wider model increases the embedding size to 512 and substantially widens the intermediate feed-forward layers freom 2 to 4. The specific parameters for each configuration can be seen in Table \ref{tab:configurations}. Across all configurations, the rest of the pipeline is unchanged. We vary only these architectural parameters in order to isolate and observe their impact on the model performance.

\begin{table}[htb]
  \centering
  \caption{Transformer architecture configurations}
  \label{tab:configurations}
  \begin{tabular}{lllll}
    \hline
    \textbf{Configuration} & \textbf{\shortstack{Embed\\Size}} &
      \textbf{\shortstack{Enc./Dec.\\Layers}} &
      \textbf{Heads} &
      \textbf{\shortstack{Forward\\Expansion}} \\
    \hline
    Baseline & 256 & 2 & 8 & 2 \\
    Small    & 128 & 2 & 4 & 2 \\
    Deep     & 256 & 4 & 8 & 2 \\
    Wide     & 512 & 2 & 8 & 4 \\
    \hline
  \end{tabular}
\end{table}



\subsection{Fully Non-linear QVK Projections}

For our second modification, we implement fully non-linear QVK projections. In the Transformer implementation from the original paper, the query (Q), key (K), and value (V) vectors are obtained by applying a single linear layer to the input embeddings. That is, each token representation is projected once to produce Q, once to produce K, and once to produce V. This keeps the attention module relatively simple, but it also means that the transformations used to construct Q, K, and V are purely linear. In the original code provided for this assignment, the query and key vectors have a two-layer MLP of the form Linear $\Rightarrow$ ReLU $\Rightarrow$ Linear, while the value vector remains purely linear.

To make this fully non-linear, we modified the multi-head attention classes in the \texttt{transformer\_encoder.py} and \texttt{transformer\_decoder.py} files so that the two-layer MLP, as described in the previous paragraph, is applied to the value vector. Additionally, we provided a command line flag \texttt{v\_is\_twolayer} that controls whether or not the two-layer MLP is applied to the value vector. This means that before a vector participates in attention as a query, key, or value, it is passed through a non-linear transformation that can model more complex relationships than a single matrix multiplication.

This modification increases the expressiveness of the attention layers. Instead of computing attention scores over linearly transformed embeddings, the model now first passes each token through a small non-linear network specific to its role (query, key, or value). This gives each head additional capacity to reshape the representation space before compatibility scores are computed, potentially enabling richer attention patterns and more informative value updates.

\subsection{Dynamic K-means Clustering}

For our final modification, we implement dynamic K-means clustering. In the Transformer implementation from the original paper, the set of motion anchors is computed once offline using K-means clustering over the training trajectories. These cluster centers are then stored as a fixed tensor called \texttt{motion\_modes} and used throughout training and evaluation as static prototypes of future motion. The model learns to select and refine these precomputed anchors, but the anchors themselves never change; they do not adapt as the network parameters or data distribution evolve.

To implement the dynamic K-means clustering, we modified the anchor handling in \texttt{model3.py} to make the anchors learnable and dynamic while still initializing them from K-means. When the user passes the \texttt{--use\_dynamic\_clustering} CLI flag, the first time the model's forward pass is called it creates a parameter self.anchors by cloning the precomputed \texttt{motion\_modes} tensor and wrapping it in nn.Parameter. Now, during future iterations during training, the model no longer uses the raw, precomputed \texttt{motion\_modes} tensor and instead uses the self.anchors tensor, which is updated through gradient descent like any other weight in the network. This means that K-means initializes the anchors, but the final anchor values are updated through backpropagation, instead of remaining static.

When the \texttt{--use\_dynamic\_clustering} CLI flag is not passed, the model falls back to the original behavior and treats \texttt{motion\_modes} as a fixed, non-trainable set of K-means centroids.

\section{Results}

In this section, we will analyze the results of our modification using various metrics, including minimum average displacement error (minADE) and minimum final displacement error (minFDE).

\subsection{Transformer Architecture}

Modifying the model architecture shows better overall perforamnce for wider models over both smaller and deeper ones. When comparing the four base configurations without dynamic clustering or no fully non-linear QVK, the wide model achieves the lowest mean ADE and FDE, slightly outperforming the baseline while using the same number of layers. In contrast, the deep model does not achieve better trajectory quality with extra depth and actually degrades both ADE and FDE relative to the baseline. The small configuration, which reduces both embedding size and number of heads, performs the worst by far, without the use of dynamic clustering or fully non-linear QVK. These results can be seen in the top left subfigure of Figure \ref{fig:ade_mean} and Figure \ref{fig:fde_mean}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=3.4in]{../images/ade_mean_2x2.png}
    \caption{The average minADE for each model configuration. The configuration with the smallest error is colored red.}
    \label{fig:ade_mean}
\end{figure}

\subsection{QVK Projections Fully Non-linear}

Enabling fully non-linear QVK projections has an effect on performance that depends strongly on the underlying architecture. For the wide model, adding fully non-linear Q, K, and V slightly improves mean ADE compared to its linear counterpart and roughly matches its FDE, making the wide architecture with fully non-linear QVK (wide\_fnl) the strongest configurations overall. For the small architecture, fully non-linear QVK clearly helps, as both ADE and FDE improve relative to the small architecture without the change. However, the small architecture is still the weakest of the four.

However, for the baseline and deep models, fully non-linear QVK ends up actually harming the results. Both baseline\_fnl and deep\_fnl show slightly higher ADE and FDE than their linear-QVK counterparts, even though the rest of the architecture is unchanged. This suggests that simply increasing the complexity of QVK mappings does not guarantee an improvement. In models of this size, it may lead to overfitting. When dynamic clustering is also enabled, the pattern persists. The small architecture improves signifigantly, but the other three architectures show worse performance compared to those architectures without those modifications enabled.

Overall, fully non-linear QVK is most effective in the small and wide architectures, but it do not improve performance across all model sizes. These results can be seen in the bottom left subfigure of Figure \ref{fig:ade_mean} and Figure \ref{fig:fde_mean}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=3.4in]{../images/fde_mean_2x2.png}
    \caption{The FDE mean for each model configuration. The configuration with the smallest error is colored red.}
    \label{fig:fde_mean}
\end{figure}

\subsection{Dynamic K-means Clustering}

Enabling dynamic clustering yields mixed results. For the baseline and wide architectures, switching from static K-means anchors to learnable anchors generally leads to slightly higher ADE and FDE. In contrast, the deep architecture actually benefits somewhat from dynamic clustering. It achieves lower ADE and FDE than the normal deep architectures, suggesting that the additional flexibility in anchor locations can help deeper networks make better use of their capacity.

The most positive impact of dynamic clustering appears in the small configurations. Both the small architectures with dynamic clustering and with both dynamic clustering and fully non-linear QVK perform substantially better than the normal small architecture in terms of both ADE and FDE. When combined with fully non-linear QVK, dynamic clustering also modestly improves the deep architecture, but it still does not allow deep architecture configurations to surpass the wide architectures. In summary, dynamic K-means clustering in its current form is beneficial primarily for the smallest and deepest models, while for well-scaled wide and baseline architectures it provides little advantage and can slightly degrade performance. These results can be seen in the top right subfigure of Figure \ref{fig:ade_mean} and Figure \ref{fig:fde_mean}.

\subsection{Cumulative Distribution Analysis}

In this section, we do a brief analysis of the cumulative distribution graphs of two model configurations. To simplify our analysis, we chose to look at the cumulative distribution for the overall worst configuration, the small architecture (Figure \ref{fig:small_cumulative_distribution}), and the overall best configuration, the wide architecture with fully non-linear QVK (Figure \ref{fig:wide_fnl_cumulative_distribution}).

The cumulative distributions for the small architecture without dynamic clustering or fully non-linear QVK show that its errors are both larger on average and significantly more spread out. The shape of these curves reflects a model that not only has worse mean performance but also suffers from a substantial number of difficult or failure cases. In contrast, the cumulative distributions for the wide architecture with fully non-linear QVK are both steeper and more compact. The ADE CDF plot goes up much more quickly toward 1, meaning that a larger percentage of trajectories achieve a low average error. The FDE CDF plot shows similar behavior. Overall, Figure \ref{fig:wide_fnl_cumulative_distribution} shows that wide\_fnl  not only improves mean ADE and FDE, but also tightens the error distribution, resulting in more reliable predictions across the testing dataset.

\begin{figure}[tb]
    \centering
    \includegraphics[width=3.3in]{../images/small.png}
    \caption{The worst overall model (the small configuration).}
    \label{fig:small_cumulative_distribution}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=3.3in]{../images/wide_fnl.png}
    \caption{The best overall model (the wide fully non-linear configuration).}
    \label{fig:wide_fnl_cumulative_distribution}
\end{figure}

\section{Conclusion}

\end{document}
